*теорвер* - закономерности в случ явлениях
*случ событие* - фаакт который может произойти или не произойти _(A)_
*вероятность события* - число характеризующее степень объективной возможности появлени событий _(P(A))_
вероятность(аксиоматический на основе теории множеств)
	множестов всех исходов - каждый элемент - элементарнное событие
	Ω - пространстов элементарных событий
	событие A - некотрое подмножество Ω
		*Достоверное* - происходит в каждом опыте
		*Невозможное* - не произойдет
		*Несовместным* - события которые в одном опыте проиойти одновременно не могут
=
сумма двух событий А и B (A+B) - событие которое заключается в том что происходит хотя бы одно из событий (A|B)
произведение(пересечение A * B) - событие когда происходят два вместе (A&B)
потивоположное к A - событие B (P(a) = P(U B_i)=1-P(A)=1-P(B_i)=1-П(1-P(b_i))) заключается в том, что событе А не происходит
А_k - полная группа если они попарно несовместны и в сумме образуют достоверное событие


A + !A = Ω; A*!A=Ø; A+Ω=Ω; A*Ω=A; A*Ø=Ø; A+Ø=A; !(A+B)=!A*!B; !(A*B)=!A+!B; A+!A*B=A+B
Аксиомы теорвер
	пусть каждомау А ставится число - вероятность события P(A)
	1) 0<=P(A)<=1
	2) P(A+B)=P(A)+P(B) - если A и B несовместные (обобщается на любое число событий, если попарно несовместны)
	A_1 A_n - равновозможны, если P(A_1)=P(A_n);
	Если в опыте Ω - представима в виде группы несовместных равновохможных событий, но такие события - случаи, опыт - схема случаев
	w_i - благоприятное событие - если он элемент множества А

Классическое определение P(A)=m/n (1.4) n-число элеметарных равновозможных исходов m- число равновозмоныхх исходов приводящих к появлению события
Геометрическое в некоторую область случайным образом бросется точка Т P(A)=S(A)/S(Ω) - S() -> площадь области (1.5)
*Выборка (n,r)* - множество , состоящее из r элементов из множества X={x_1,..,x_n}
*Упорядоченная выборка* - с порядком следования P(n,r)=n!/(n,r)!; (1.7)
Если несколько раз извлекли - выборка с повторением  P^(n,r)=n^r; (1.6)
Число неупорядоченных выборок С|r,n = n!/(r!(n,r)!); (1.9)
с повоторениями C^|r,n=(n+r-1)/(r!(n,r)!); (1.8)
Число различных разбиение мн-ва из n элементов на k непересекающихся множеств n = r1+..r_k
Pn(r1,..rk)=n!/(r1!r2!..rk!) (1.10);

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

*несовместимые* события - если появление одного из них исключает возможность появления остальных {1.11}
Событие A называется *независимым* от события B, если возможность наступления события A не зависит от того, произошло событие B или нет. P(B/A) = P(B)

*Условной вероятностью* события B при наличии A называется величина {1.17}
Условную вероятность события P(B/A) можно трактовать как вероятность события B , вычисленная при условии, что событие A произошло .

*Вероятность произведения двух событий* равна вероятности одного из них, умноженной на условную вероятность второго при наличии первого (правило умножения вероятностей).{1.18}
*вероятность произведения нескольких событий* равна произведению вероятностей этих событий, причем вероятность каждого последующего события вычисляется при условии, что все предыдущие имели место{1.19}
*вероятность произведения нескольких независимых событий* равна произведению вероятностей этих событий
попарная независимость не гарантирует совокупной независимости

Каждая *гипотеза* осуществляется случайным образом и представляет собой некоторые события, вероятности которых известны {1.21}
*полная вероятность события A* вычисляется как сумма произведений вероятности каждой гипотезы на условную вероятность события при этой гипотезе.

Следствием правила умножения, и формулы полной вероятности является *теорема гипотез или формула Байеса*
	послеопытные вероятности называются *апостериорными*
	позволяет пересчитать вероятности гипотез в свете новой информации, состоящей в том, что опыт дал результат А

Несколько опытов называются *независимыми опытами*, если вероятность исхода опыта не зависит от того, какие исходы имели другие опыты.
Число к0, которому соответствует максимальная биномиальная вероятность   , называется *наивероятнейшим числом* появления события А. При заданных n и p {1.30}

Под *случайной величиной* понимается величина, которая в результате опыта со случайным исходом принимает то или иное значение
Возможные значения случайной величины образуют множество Ξ, которое называется *множеством возможных значений случайной величины*.
*Дискретные* если множество Ξ счетное и конечное
*Недискретная* - Ξ несчетно
*Законом распределения* СВ называется любое правило (таблица, функция), позволяющее находить вероятности всевозможных событий, связанных со случайной величиной.
(То есть, всякое соотношение, устанавливающее связь между возможными значениями СВ и их вероятностями.)
СВ - *подчинена данному закону распределения*

*Ряд распределения* дискретной случайноу величины - называется таблица в котором в порядке возрастания перечислены все значения X и
вероятности этой величины P (вероятность того что в результате опыта СВ примет значение X)

*Многоугольник вероятностей* - графическое отображение ряда распределения дискретной случайной величины

*Функция распределения* - наиболее общая форма пригодная для всех СВ
ФР СВ - X называется вероятность того, что она примет значение меньшее, чем аргумент функции F(x)=P { X < x }
Вероятность того, что случайная величина Х в результате опыта попадет на участок от α до β (включая α) равна приращению функции распределения на этом участке.
функция распределения F(x)любой случайной величины есть неубывающая функция своего аргумента, значения которой заключены между 0 и 1: 0≤F(x)≤1, причем F(-∞)=0, F(+∞)=1.

ФР дискретной СВ - разрывная ступенчатая

СВ - *непрерывная*, если F(x) - непрерывная функция, кусочно-деффиренцируемая
вероятность любого отдельного значения непрерывной случайной величины равна нулю

*Плотность распределения* - (или плотностью вероятности) непрерывной случайной величины X в точке x называется производная ее функции распределения в этой точке и обозначается f (x )
Вероятность попадания случайной величины X на интервал от x до x+dx равна f (x )dx . Эта величина называется *элементом вероятности*
Вероятность попадания случайной величины X на произвольный участок [a , b ] равна сумме элементарных вероятностей на этом участке (интеграл от f(x)dx по a/b)

*числовыe характеристики случайной величины*
	МО - *математическое ожидание* характеризует среднее взвешенное значение случайной величины.
	Физический смысл МО – это среднее значение случайной величины, то значение, которое  может быть использовано вместо конкретного значения, принимаемого случайной величиной  в приблизительных расчетах или оценках.
	*Момент* - Начальный момент -s-го порядка СВ X есть математическое ожидание s-й степени этой случайной величины
	МО - начальный момент 1 порядка
	*Центрированной СВ* - X0=X-m_x
	*Центральным моментом* s-го порядка СВ X есть математическое ожидание s-й степени центрированной случайной величины
	*Дисперсия* случайной величины есть математическое ожидание квадрата соответствующей центрированной случайной величины.
	Она характеризует степень  разброса значений случайной величины относительно ее математического ожидания, т.е. ширину диапазона значений.
	Дисперсия СВ (как дискретной, так и непрерывной) есть неслучайная (постоянная) величина.
	Дисперсия СВ имеет размерность квадрата случайной величины
	*СКО* измеряется в тех же физических единицах, что и СВ, и характеризует ширину диапазона значений СВ.
	*Правило 3s*. Для большинства значений случайной величины  абсолютная величина ее отклонения от математического ожидания не превосходит утроенного среднего квадратического отклонения, другими словами,
	практически все значения СВ находятся в интервале: [m-3s;m+3s]
	*Модой* случайной величины называется ее наиболее вероятное значение, т.е. то значение, для которого вероятность pi (для дискретной СВ) или f(x) (для непрерывных СВ) достигают максимума. Обозначения: Mx, Mo.:
	*«унимодальным»* называется Распределение с одним максимумом  ряда распределения (для ДСВ) или плотности вероятности (НСВ)
	несколько максимумов - *полимодальный*
	если распределение обладает только минимумом - *антимодальное*
	*Медианой* случайной величины X называется такое ее значение, для  ко­торого выполняется условие P{X<Me} = P{X³Me}. Медиана, как  правило,  существует  только  для непрерывных случайных величин:
квантилью c_p случайной величины Х называется
	Третий *центральный момент* служит для характеристики асимметрии распределения.
		Если распределение симметрично относительно МО (масса распределена равномерно относительно центра тяжести), то все моменты нечетного порядка равны нулю
		Поэтому для характеристик асимметрии выбирают третий центральный момент – он имеет размерность куба случайной величины.
	Четвертый центральный момент служит для характеристики так называемой «крутости», т.е. островершинности или плосковершинности распределения. Это свойство распределения описывается с помощью *эксцесса*
	Коэффициент *вариации* безразмерная величина, характеризует степень разбросанности значений СВ

*системоой случайных величин* (случайным вектором, многомерной случайной величиной) называется любая упорядоченная совокупность случайных величин Х ={ X 1 , …, X n }.
	*Функцией распределения* (или совместной функцией распределения ) системы случайных величин называется вероятность совместного выполнения неравенств   X 1 < x 1 , …, X n < x n
	системы двух случайных величин представляет собой  некоторую *поверхность распределения*
	*Элемент вероятности* - f(x,y)dxdy с точностью до бесконечно малых величин равен вероятности попадания случайной точки ( X , Y ) в элементарный прямоугольник R примыкающий к точке (x,y), с размерами dx dy
	Одномерные плотности распределения составляющих системы случайных величин называют *маргинальными плотностями распределения*.
	*Условным законом распределения* называется распределение одной случайной величины, найденное при условии, что другая случайная величина приняла определенное значение.
	св независимы, если x1, x2, x3 не зависят от того, какие значения приняли другие остальные СВ
	Особую роль играет центральный момент порядка 1+1 или второй смешанный центральный момент, который называется   *ковариацией* или *корреляционным моментом*
	Свойства корреляции:
		1)k_xy=k_yx
		2)Корреляционный момент двух независимых случайных величин Х и Уравен нулю
		3)Абсолютная величина корреляционного момента двух случайных величин не превышает среднего геометрического их дисперсий
	*Коэффициент корреляции* служит для оценки тесноты линейной связи между Хи Y : чем ближе абсолютная величина коэффициента корреляции к 1, тем связь сильнее, чем ближе к 0, тем слабее.

числовые характеристики систем случайных величин:
	*Вектор математических ожиданий* - характеризующий среднее значение величин
	*Вектор дисперсий* - характеризующий их рассеивание
	*Корреляционная матрица* - попарная корреляция входящих в систему величин, на диагонали которой будут значения дисперсии (K_ii=D=M[x_i^2]
	*матрица коэффициентов корреляции*.составленная коэффициентов корреляции
условыне ч/х ССВ
*Условным математическим ожиданием* компоненты Х называется математическое ожидание СВ Х, вычисленное  при условии, что  СВ Y приняла определенное значение Y=y и обозначается М(Х/ Y)
Условное математическое ожидание СВ Y при заданном X = x : M[ Y / x ]= m y / x называется *регрессией* Y на x
Регрессионный анализ позволяет выявить характер связи между величинами









